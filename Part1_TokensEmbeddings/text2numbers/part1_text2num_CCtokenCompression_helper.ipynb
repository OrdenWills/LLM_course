{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fMIX8VSYc2Tm"
   },
   "source": [
    "|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n",
    "|-|:-:|\n",
    "|<h2>Part 1:</h2>|<h1>Tokenizations and embeddings<h1>|\n",
    "|<h2>Section:</h2>|<h1>Words to tokens to numbers<h1>|\n",
    "|<h2>Lecture:</h2>|<h1><b>CodeChallenge HELPER: Tokenization compression ratios<b></h1>|\n",
    "\n",
    "<br>\n",
    "\n",
    "<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n",
    "<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n",
    "<i>Using the code without the course may lead to confusion or errors.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0GmDlHtoc2Or"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "C8JzcByJ_dVt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in c:\\users\\adolphus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.12.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\adolphus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tiktoken) (2025.11.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\adolphus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tiktoken) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\adolphus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\adolphus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\adolphus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\adolphus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2025.8.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# for getting text data off the web\n",
    "import requests\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# strings\n",
    "import string\n",
    "\n",
    "!pip install tiktoken\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "f34QAbA-_gD-"
   },
   "outputs": [],
   "source": [
    "# GPT-4's tokenizer\n",
    "tokenizer = tiktoken.get_encoding('cl100k_base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vll_zH5t_gHC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nsfMaYxen8oU"
   },
   "source": [
    "# Exercise 1: The books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "fX_ZgpveSYPL"
   },
   "outputs": [],
   "source": [
    "# all books have the same url format;\n",
    "# they are unique by numerical code\n",
    "baseurl = 'https://www.gutenberg.org/cache/epub/'\n",
    "\n",
    "bookurls = [\n",
    "    # code       title\n",
    "    ['84',    'Frankenstein'    ],\n",
    "    ['64317', 'GreatGatsby'     ],\n",
    "    ['11',    'AliceWonderland' ],\n",
    "    ['1513',  'RomeoJuliet'     ],\n",
    "    ['76',    'HuckFinn'        ],\n",
    "    ['219',   'HeartDarkness'   ],\n",
    "    ['2591',  'GrimmsTales'     ],\n",
    "    ['2148',  'EdgarAllenPoe'   ],\n",
    "    ['36',    'WarOfTheWorlds'  ],\n",
    "    ['829',   'GulliversTravels']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "AW2MzjfdRhB8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Book title     |  Chars  |  Tokens | Compression\n",
      "--------------------------------------------------\n",
      "Frankenstein     | 446,544 | 102,419 |  22.94%\n",
      "GreatGatsby      | 296,858 |  70,343 |  23.70%\n",
      "AliceWonderland  | 167,674 |  41,457 |  24.72%\n",
      "RomeoJuliet      | 167,429 |  43,761 |  26.14%\n",
      "HuckFinn         | 602,714 | 159,125 |  26.40%\n",
      "HeartDarkness    | 232,885 |  56,483 |  24.25%\n",
      "GrimmsTales      | 549,736 | 137,252 |  24.97%\n",
      "EdgarAllenPoe    | 632,136 | 144,315 |  22.83%\n",
      "WarOfTheWorlds   | 363,420 |  84,580 |  23.27%\n",
      "GulliversTravels | 611,742 | 143,560 |  23.47%\n"
     ]
    }
   ],
   "source": [
    "print('  Book title     |  Chars  |  Tokens | Compression')\n",
    "print('-'*50)\n",
    "\n",
    "for code,title in bookurls:\n",
    "\n",
    "  # get the text\n",
    "  fullurl = f'{baseurl}{code}/pg{code}.txt'\n",
    "  text = requests.get(fullurl).text\n",
    "  num_chars = len(text)\n",
    "\n",
    "  # tokenize\n",
    "  tokens = tokenizer.encode(text)\n",
    "  num_tokens = len(tokens)\n",
    "\n",
    "  # compression ratio\n",
    "  compress = 100 * num_tokens / num_chars\n",
    "\n",
    "  print(f'{title:16} | {num_chars:>7,d} | {num_tokens:>7,d} |  {compress:>3.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5G8IwDojRg_E"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0FZ6gF8zRg8H"
   },
   "source": [
    "# Exercise 2: Repeat with websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "KOTAgq4PXMVc"
   },
   "outputs": [],
   "source": [
    "weburls = [\n",
    "    'http://python.org/',\n",
    "    'https://pytorch.org/',\n",
    "    'https://en.wikipedia.org/wiki/List_of_English_words_containing_Q_not_followed_by_U',\n",
    "    'https://sudoku.com/',\n",
    "    'https://reddit.com/',\n",
    "    'https://visiteurope.com/en/',\n",
    "    'https://sincxpress.com/',\n",
    "    'https://openai.com/',\n",
    "    'https://theuselessweb.com/',\n",
    "    'https://maps.google.com/',\n",
    "    'https://pigeonsarentreal.co.uk/',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "hC7ZulNxRg5K"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Website        |  Chars  |  Tokens | Compression\n",
      "-----------------------------------------------------\n",
      "python             |  49,708 |  12,648 |  25.44%\n",
      "pytorch            | 191,088 |  58,969 |  30.86%\n",
      "en.wikipedia       |     126 |      34 |  26.98%\n",
      "sudoku             | 140,759 |  51,332 |  36.47%\n",
      "reddit             | 448,668 | 143,441 |  31.97%\n",
      "visiteurope        | 402,251 | 154,476 |  38.40%\n",
      "sincxpress         |  25,580 |   6,843 |  26.75%\n",
      "openai             |  11,575 |   6,433 |  55.58%\n",
      "theuselessweb      |   4,756 |   1,329 |  27.94%\n",
      "maps.google        | 212,445 | 107,845 |  50.76%\n",
      "pigeonsarentreal.c |   7,300 |   2,052 |  28.11%\n"
     ]
    }
   ],
   "source": [
    "print('    Website        |  Chars  |  Tokens | Compression')\n",
    "print('-'*53)\n",
    "\n",
    "for url in weburls:\n",
    "\n",
    "  # get the text\n",
    "  text = requests.get(url).text\n",
    "  num_chars = len(text)\n",
    "\n",
    "  # tokenize\n",
    "  tokens = tokenizer.encode(text)\n",
    "  num_tokens = len(tokens)\n",
    "\n",
    "  # compression ratio\n",
    "  compress = 100 * num_tokens / num_chars\n",
    "\n",
    "  print(f'{urlparse(url).hostname[:-4]:18} | {num_chars:>7,d} | {num_tokens:>7,d} |  {compress:>3.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RczL4_fIWVBK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xOShW7tBhftK"
   },
   "source": [
    "# Exercise 3: Using the 'string' library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "i_3jPwujPEpV"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['string',\n",
       " 'A collection of string constants.\\n\\nPublic module variables:\\n\\nwhitespace -- a string containing all ASCII whitespace\\nascii_lowercase -- a string containing all ASCII lowercase letters\\nascii_uppercase -- a string containing all ASCII uppercase letters\\nascii_letters -- a string containing all ASCII letters\\ndigits -- a string containing all ASCII decimal digits\\nhexdigits -- a string containing all ASCII hexadecimal digits\\noctdigits -- a string containing all ASCII octal digits\\npunctuation -- a string containing all ASCII punctuation characters\\nprintable -- a string containing all ASCII characters considered printable\\n\\n',\n",
       " 'C:\\\\Users\\\\Adolphus\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\Lib\\\\string.py',\n",
       " 'C:\\\\Users\\\\Adolphus\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\Lib\\\\__pycache__\\\\string.cpython-312.pyc',\n",
       " ' \\t\\n\\r\\x0b\\x0c',\n",
       " 'abcdefghijklmnopqrstuvwxyz',\n",
       " 'ABCDEFGHIJKLMNOPQRSTUVWXYZ',\n",
       " 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ',\n",
       " '0123456789',\n",
       " '0123456789abcdefABCDEF',\n",
       " '01234567',\n",
       " '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~',\n",
       " '0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~ \\t\\n\\r\\x0b\\x0c']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# e.g.,\n",
    "string.__dict__['__doc__']\n",
    "stringss = string.ascii_lowercase\n",
    "[j for j in string.__dict__.values() if type(j) == str and len(j)> 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "pozOkYnVfwSI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Attribute       |  Chars  |  Tokens | Compression\n",
      "--------------------------------------------------\n",
      "__name__          |       6 |       1 |  16.67%\n",
      "__doc__           |     622 |     109 |  17.53%\n",
      "__file__          |      71 |      22 |  30.99%\n",
      "__cached__        |      96 |      32 |  33.33%\n",
      "whitespace        |       6 |       4 |  66.67%\n",
      "ascii_lowercase   |      26 |       1 |  3.85%\n",
      "ascii_uppercase   |      26 |       1 |  3.85%\n",
      "ascii_letters     |      52 |       2 |  3.85%\n",
      "digits            |      10 |       4 |  40.00%\n",
      "hexdigits         |      22 |       7 |  31.82%\n",
      "octdigits         |       8 |       3 |  37.50%\n",
      "punctuation       |      32 |      21 |  65.63%\n",
      "printable         |     100 |      31 |  31.00%\n"
     ]
    }
   ],
   "source": [
    "print('  Attribute       |  Chars  |  Tokens | Compression')\n",
    "print('-'*50)\n",
    "\n",
    "for k,v in string.__dict__.items():\n",
    "  # print(v)\n",
    "  if type(v) == str and len(v) >= 1:\n",
    "\n",
    "    # get the text\n",
    "    text = v\n",
    "\n",
    "    # tokenize\n",
    "    # print(k)\n",
    "    # print('-------------------------')\n",
    "    tokens = tokenizer.encode(text)\n",
    "    # print(tokens)\n",
    "    num_tokens = len(tokens)\n",
    "\n",
    "    # compression ratio\n",
    "    # print(len(text))\n",
    "\n",
    "    compress = 100 * num_tokens / len(text)+ .001\n",
    "\n",
    "    # print the results\n",
    "    print(f'{k:18}| {len(text):>7,d} | {num_tokens:>7,d} |  {compress:>3.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LmF_zWxheB1F"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPCu3ErzRpYhUHCAoMYJIgw",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
