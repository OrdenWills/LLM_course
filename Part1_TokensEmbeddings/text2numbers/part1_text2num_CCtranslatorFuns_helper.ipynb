{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cPXS7dDYcImb"
   },
   "source": [
    "|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n",
    "|-|:-:|\n",
    "|<h2>Part 1:</h2>|<h1>Tokenizations and embeddings<h1>|\n",
    "|<h2>Section:</h2>|<h1>Words to tokens to numbers<h1>|\n",
    "|<h2>Lecture:</h2>|<h1><b>CodeChallenge HELPER: More on token translation<b></h1>|\n",
    "\n",
    "<br>\n",
    "\n",
    "<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n",
    "<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n",
    "<i>Using the code without the course may lead to confusion or errors.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qqTos9Wpuq1-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1XnNp0fLuqyh"
   },
   "source": [
    "# Import two tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "QnBjS3LAujoq"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "c:\\Users\\Adolphus\\projects\\LLM_course\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in c:\\users\\adolphus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.12.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\adolphus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tiktoken) (2025.11.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\adolphus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tiktoken) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\adolphus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\adolphus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\adolphus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\adolphus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2025.8.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken\n",
    "import tiktoken\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "wJfxECXYusgP"
   },
   "outputs": [],
   "source": [
    "bertTokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "gpt4Tokenizer = tiktoken.get_encoding('cl100k_base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T7DbSkSquqp5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ur-eP9j7uqm8"
   },
   "source": [
    "# Exercise 1: Write translation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "PjXLN6TTwcQ6"
   },
   "outputs": [],
   "source": [
    "# translation functions\n",
    "def bert2gpt4(bertToks):\n",
    "  b = bertTokenizer.decode(bertToks, skip_special_tokens=True)# decode\n",
    "  g = gpt4Tokenizer.encode(b) # encode\n",
    "  return g\n",
    "\n",
    "def gpt42bert(gpt4Toks):\n",
    "  g = gpt4Tokenizer.decode(gpt4Toks)\n",
    "  b = bertTokenizer.encode(g) # encode\n",
    "  return b[1:-1] # something to exclude special tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "MXpmFE3cBfPP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i wish chocolate were purple.\n",
      "i wish chocolate were purple.\n"
     ]
    }
   ],
   "source": [
    "# just checking that it gives no errors\n",
    "text = 'I wish chocolate were purple.'\n",
    "\n",
    "print(gpt4Tokenizer.decode(bert2gpt4(bertTokenizer.encode(text))))\n",
    "print(bertTokenizer.decode(gpt42bert(gpt4Tokenizer.encode(text))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "txY-ShywAJ_S"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L4Du3JPXAJ6p"
   },
   "source": [
    "# Exercise 2: BERT --> GPT4 --> BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "lkf2RFVk76xH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text\n",
      "  I wanted to paste in a thought-provoking quote here, but I didn't.\n",
      "\n",
      "BERT tokens:\n",
      "  [101, 1045, 2359, 2000, 19351, 1999, 1037, 2245, 1011, 4013, 22776, 14686, 2182, 1010, 2021, 1045, 2134, 1005, 1056, 1012, 102]\n",
      "\n",
      "BERT to GPT4:\n",
      "  i wanted to paste in a thought - provoking quote here, but i didn't.\n",
      "\n",
      "Back to BERT:\n",
      "  i wanted to paste in a thought - provoking quote here, but i didn't.\n"
     ]
    }
   ],
   "source": [
    "# sample text\n",
    "text = \"I wanted to paste in a thought-provoking quote here, but I didn't.\"\n",
    "print(f'Original text\\n  {text}\\n')\n",
    "\n",
    "# initial encoding\n",
    "bertTox = bertTokenizer.encode(text)\n",
    "print(f'BERT tokens:\\n  {bertTox}\\n')\n",
    "\n",
    "# translate to GPT4\n",
    "b2g = bert2gpt4(bertTox)\n",
    "print(f'BERT to GPT4:\\n  {gpt4Tokenizer.decode(b2g)}\\n')\n",
    "\n",
    "# back-translate to BERT\n",
    "back2bert = gpt42bert(b2g)\n",
    "print(f'Back to BERT:\\n  {bertTokenizer.decode(back2bert)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-WumOSErA5pJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ipwIs91jA5mB"
   },
   "source": [
    "# Exercise 3: GPT4 --> BERT --> GPT4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "G1bpWEnT33Es"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text\n",
      "  i still don't have a good quote here. now it's too late.\n",
      "\n",
      "Initial encoding\n",
      "  [72, 2103, 1541, 956, 617, 264, 1695, 12929, 1618, 13, 1457, 433, 596, 2288, 3389, 13]\n",
      "\n",
      "GPT4 to BERT\n",
      "  [1045, 2145, 2123, 1005, 1056, 2031, 1037, 2204, 14686, 2182, 1012, 2085, 2009, 1005, 1055, 2205, 2397, 1012]\n",
      "\n",
      "Decoded text is : i still don't have a good quote here. now it's too late.\n",
      "BERT to GPT4\n",
      "  [72, 2103, 1541, 956, 617, 264, 1695, 12929, 1618, 13, 1457, 433, 596, 2288, 3389, 13]\n",
      "\n",
      "Decoded text is : i still don't have a good quote here. now it's too late.\n"
     ]
    }
   ],
   "source": [
    "# sample text\n",
    "text = \"I still don't have a good quote here. Now it's too late.\".lower()\n",
    "print(f'Original text\\n  {text}\\n')\n",
    "\n",
    "# initial encoding\n",
    "initEnc = gpt4Tokenizer.encode(text)\n",
    "print(f'Initial encoding\\n  {initEnc}\\n')\n",
    "\n",
    "# translate to BERT\n",
    "t2 = gpt42bert(initEnc)\n",
    "print(f'GPT4 to BERT\\n  {t2}\\n')\n",
    "print(f\"Decoded text is : {bertTokenizer.decode(t2, skip_special_tokens=True)}\")\n",
    "\n",
    "# back-translate to GPT4\n",
    "t3 = bert2gpt4(t2)\n",
    "print(f'BERT to GPT4\\n  {t3}\\n')\n",
    "print(f\"Decoded text is : {gpt4Tokenizer.decode(t3)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "309X7W4O33Bi"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO+3GYaK7YmfsINQR8Q23GI",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
